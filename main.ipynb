{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3\n",
    "%pip install pandas\n",
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import urllib.request\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the necessary folders to manipulate the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Delete _datasets folder if exists.\n",
      "Create _datasets _datasets/csv _datasets/parquet _datasets/zipped.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"\" \n",
    "echo \"Delete _datasets folder if exists.\"\n",
    "rm -rf _datasets\n",
    "echo \"Create _datasets _datasets/csv _datasets/parquet _datasets/zipped.\"\n",
    "mkdir _datasets\n",
    "mkdir _datasets/csv\n",
    "mkdir _datasets/parquet\n",
    "mkdir _datasets/zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a function to extract ZIP from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_enem(year : str) -> bool:\n",
    "    \"\"\"\n",
    "    Extract data about Exame Nacional do Ensino Médio (ENEM) during an specific given period.\n",
    "\n",
    "    Args: \n",
    "        year (str): given period.\n",
    "\n",
    "    Returns:\n",
    "        False if any error has been generated during downloading process. \n",
    "        True if no error has been generated during downloading process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://download.inep.gov.br/microdados/microdados_enem_{year}.zip\".format(year=year)\n",
    "        output = \"_datasets/zipped/micro-dados-enem-{year}.zip\".format(year=year)\n",
    "        print(f\"\\nDownloading zipped file from {url}...\")\n",
    "        urllib.request.urlretrieve(url, output)\n",
    "        print(f\"Zipped file wrote in {output}.\")\n",
    "        return True\n",
    "    except:\n",
    "        print(\"\\nError during zipped file downloading proccess.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the ZIP file using the function created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading zipped file from https://download.inep.gov.br/microdados/microdados_enem_2020.zip...\n",
      "Zipped file wrote in _datasets/zipped/micro-dados-enem-2020.zip.\n",
      "\n",
      "Downloading zipped file from https://download.inep.gov.br/microdados/microdados_enem_2021.zip...\n",
      "Zipped file wrote in _datasets/zipped/micro-dados-enem-2021.zip.\n",
      "\n",
      "Downloading zipped file from https://download.inep.gov.br/microdados/microdados_enem_2022.zip...\n",
      "Zipped file wrote in _datasets/zipped/micro-dados-enem-2022.zip.\n",
      "\n",
      "Downloading zipped file from https://download.inep.gov.br/microdados/microdados_enem_2023.zip...\n",
      "Zipped file wrote in _datasets/zipped/micro-dados-enem-2023.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True, True, True, True]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[extract_data_from_enem(year=year) for year in [\"2020\", \"2021\", \"2022\", \"2023\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unzip the CSV file from ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unzipping file micro-dados-enem-2020.zip from _datasets/zipped...\n",
      "Moving file micro-dados-enem-2020.csv to _datasets/csv...\n",
      "CSV file moved to _datasets/csv/micro-dados-enem-2020.csv.\n",
      "\u001b[H\u001b[2J\n",
      "Unzipping file micro-dados-enem-2021.zip from _datasets/zipped...\n",
      "Moving file micro-dados-enem-2021.csv to _datasets/csv...\n",
      "CSV file moved to _datasets/csv/micro-dados-enem-2021.csv.\n",
      "\u001b[H\u001b[2J\n",
      "Unzipping file micro-dados-enem-2022.zip from _datasets/zipped...\n",
      "Moving file micro-dados-enem-2022.csv to _datasets/csv...\n",
      "CSV file moved to _datasets/csv/micro-dados-enem-2022.csv.\n",
      "\u001b[H\u001b[2J\n",
      "Unzipping file micro-dados-enem-2023.zip from _datasets/zipped...\n",
      "Moving file micro-dados-enem-2023.csv to _datasets/csv...\n",
      "CSV file moved to _datasets/csv/micro-dados-enem-2023.csv.\n",
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "years=()\n",
    "directory=\"_datasets/zipped/*\"\n",
    "\n",
    "for file in $directory\n",
    "do \n",
    "    filename=$(basename \"$file\")\n",
    "    filename=${filename:17:4}\n",
    "    years+=(\"$filename\")\n",
    "done\n",
    "\n",
    "for year in ${years[@]}\n",
    "do\n",
    "    echo \"\"\n",
    "    echo \"Unzipping file micro-dados-enem-$year.zip from _datasets/zipped...\"\n",
    "    unzip -qq \"_datasets/zipped/micro-dados-enem-$year.zip\"\n",
    "    echo \"Moving file micro-dados-enem-$year.csv to _datasets/csv...\"\n",
    "    mv \"DADOS/MICRODADOS_ENEM_$year.csv\" \"_datasets/csv/micro-dados-enem-$year.csv\"\n",
    "    echo \"CSV file moved to _datasets/csv/micro-dados-enem-$year.csv.\"\n",
    "    rm -rf \"DADOS\" \"DICIONÁRIO\" \"INPUTS\" \"LEIA-ME E DOCUMENTOS TÉCNICOS\" \"PROVAS E GABARITOS\"\n",
    "    clear\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the CSV to PARQUET file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(data : pd.DataFrame, filePath : str) -> None:\n",
    "    \"\"\"\n",
    "    Write a parquet file if it does not exist or append an existing parquet file.\n",
    "\n",
    "    Args: \n",
    "        data (pd.DataFrame): a dataframe from pandas.\n",
    "        filePath (str): path to write the parquet file.\n",
    "    \"\"\"\n",
    "    dataframe = pa.Table.from_pandas(data.astype(str))\n",
    "    if not os.path.exists(filePath):\n",
    "        pq.write_table(dataframe, filePath, compression=\"snappy\")\n",
    "    else:\n",
    "        existingDataframe = pq.read_table(filePath)\n",
    "        combinedDataframe = pa.concat_tables([existingDataframe, dataframe])\n",
    "        pq.write_table(combinedDataframe, filePath, compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_parquet(csvFilePath, parquetFilePath) -> bool:\n",
    "    \"\"\"\n",
    "    Read a CSV file, chunk it and write each part using the write_parquet function.\n",
    "\n",
    "    Args: \n",
    "        csvFilePath (str): directory from csv file.\n",
    "        parquetFilePath (str): directory from parquet file.\n",
    "\n",
    "    Returns:\n",
    "        False if any error has been generated during converting process. \n",
    "        True if no error has been generated during converting process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        chunksize = 1_000_000\n",
    "        \n",
    "        print(f\"\\nReading {csvFilePath} from _datasets/csv...\")\n",
    "        chunks = pd.read_csv(csvFilePath, delimiter=\";\", encoding=\"latin-1\", chunksize=chunksize)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"Converting the {i * chunksize} lines from csv to parquet...\")\n",
    "            write_parquet(chunk, parquetFilePath)\n",
    "\n",
    "        print(f\"Parquet file wrote in {parquetFilePath}.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error {e} during CSV to parquet conversion.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading _datasets/csv/micro-dados-enem-2020.csv from _datasets/csv...\n",
      "Converting the 0 lines from csv to parquet...\n",
      "Converting the 1000000 lines from csv to parquet...\n",
      "Converting the 2000000 lines from csv to parquet...\n",
      "Converting the 3000000 lines from csv to parquet...\n",
      "Converting the 4000000 lines from csv to parquet...\n",
      "Converting the 5000000 lines from csv to parquet...\n",
      "Parquet file wrote in _datasets/parquet/micro-dados-enem-2020.parquet.\n",
      "\n",
      "Reading _datasets/csv/micro-dados-enem-2021.csv from _datasets/csv...\n",
      "Converting the 0 lines from csv to parquet...\n",
      "Converting the 1000000 lines from csv to parquet...\n",
      "Converting the 2000000 lines from csv to parquet...\n",
      "Converting the 3000000 lines from csv to parquet...\n",
      "Parquet file wrote in _datasets/parquet/micro-dados-enem-2021.parquet.\n",
      "\n",
      "Reading _datasets/csv/micro-dados-enem-2022.csv from _datasets/csv...\n",
      "Converting the 0 lines from csv to parquet...\n",
      "Converting the 1000000 lines from csv to parquet...\n",
      "Converting the 2000000 lines from csv to parquet...\n",
      "Converting the 3000000 lines from csv to parquet...\n",
      "Parquet file wrote in _datasets/parquet/micro-dados-enem-2022.parquet.\n",
      "\n",
      "Reading _datasets/csv/micro-dados-enem-2023.csv from _datasets/csv...\n",
      "Converting the 0 lines from csv to parquet...\n",
      "Converting the 1000000 lines from csv to parquet...\n",
      "Converting the 2000000 lines from csv to parquet...\n",
      "Converting the 3000000 lines from csv to parquet...\n",
      "Parquet file wrote in _datasets/parquet/micro-dados-enem-2023.parquet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True, True, True, True]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[convert_csv_to_parquet(f\"_datasets/csv/micro-dados-enem-{year}.csv\", f\"_datasets/parquet/micro-dados-enem-{year}.parquet\") for year in [\"2020\", \"2021\", \"2022\", \"2023\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect with AWS using BOTO3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY\")\n",
    "AWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_KEY\")\n",
    "AWS_REGION = \"sa-east-1\"\n",
    "\n",
    "boto3.setup_default_session(\n",
    "    aws_access_key_id = AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key = AWS_SECRET_KEY,\n",
    "    region_name = AWS_REGION\n",
    ")\n",
    "\n",
    "s3 = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading micro-dados-enem-2020.parquet from _datasets/parquet to AWS bucket...\n",
      "Parquet file micro-dados-enem-2020.parquet uploaded.\n",
      "\n",
      "Uploading micro-dados-enem-2021.parquet from _datasets/parquet to AWS bucket...\n",
      "Parquet file micro-dados-enem-2021.parquet uploaded.\n",
      "\n",
      "Uploading micro-dados-enem-2022.parquet from _datasets/parquet to AWS bucket...\n",
      "Parquet file micro-dados-enem-2022.parquet uploaded.\n",
      "\n",
      "Uploading micro-dados-enem-2023.parquet from _datasets/parquet to AWS bucket...\n",
      "Parquet file micro-dados-enem-2023.parquet uploaded.\n"
     ]
    }
   ],
   "source": [
    "AWS_BUCKET_NAME = os.environ.get(\"AWS_BUCKET_NAME\")\n",
    "parquetFiles = sorted(os.listdir(\"_datasets/parquet\"))\n",
    "\n",
    "for parquet in parquetFiles:\n",
    "    print(f\"\\nUploading {parquet} from _datasets/parquet to AWS bucket...\")\n",
    "    s3.upload_file(\n",
    "        Filename = f\"_datasets/parquet/{parquet}\",\n",
    "        Bucket = AWS_BUCKET_NAME,\n",
    "        Key = f\"sor/{parquet}\"\n",
    "    )\n",
    "    print(f\"Parquet file {parquet} uploaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
